# Руководство по развертыванию локальной LLM системы

## Введение

### Что это такое и зачем нужно?

Представьте, что у вас есть:
1. Внутренняя вики с документацией
2. Код в GitLab
3. Задачи в трекере

И вы хотите, чтобы сотрудники могли задавать вопросы по всему этому контенту и получать точные ответы. Например:
- "Как настроить новый сервис?"
- "Где находится код для обработки платежей?"
- "Какие задачи связаны с модулем авторизации?"

### Основные компоненты системы

1. **LLM (Large Language Model) - DeepSeek**
   - Это "мозг" системы
   - Отвечает на вопросы
   - Понимает контекст
   - Генерирует ответы

2. **RAG (Retrieval Augmented Generation)**
   - Это система поиска релевантной информации
   - Когда сотрудник задает вопрос:
     1. RAG ищет похожие документы в вашей базе знаний
     2. Находит самые подходящие фрагменты
     3. Отправляет их в LLM для формирования ответа
   - Почему это важно:
     - LLM не хранит ваши данные
     - Ответы всегда актуальны
     - Можно указать источник информации

3. **Векторная база данных (ChromaDB)**
   - Хранит "понимание" ваших документов
   - Позволяет быстро искать похожие тексты
   - Обновляется при изменении документов

4. **Сервис чатов**
   - Веб-интерфейс для сотрудников
   - История диалогов
   - Возможность задавать уточняющие вопросы

### Как это работает?

1. **Подготовка данных:**
   - Собираем все документы из вики
   - Копируем код из GitLab
   - Экспортируем задачи из трекера
   - Разбиваем на небольшие фрагменты
   - Создаем "понимание" каждого фрагмента (эмбеддинги)
   - Сохраняем в векторную БД

2. **Когда сотрудник задает вопрос:**
   - Система ищет похожие фрагменты в базе
   - Находит самые релевантные
   - Отправляет их в LLM
   - LLM формирует ответ на основе найденной информации

3. **Обновление знаний:**
   - При изменении документов
   - При добавлении нового кода
   - При создании новых задач
   - Система автоматически обновляет свою базу знаний

### Преимущества такого подхода:

1. **Безопасность:**
   - Все данные остаются внутри компании
   - Нет отправки информации в облако
   - Контроль доступа к информации

2. **Актуальность:**
   - Ответы всегда основаны на последних данных
   - Легко обновлять знания
   - Можно указать источник информации

3. **Удобство:**
   - Единая точка доступа ко всем знаниям
   - Естественный язык запросов
   - Быстрые и точные ответы

4. **Масштабируемость:**
   - Можно добавлять новые источники данных
   - Легко обновлять модель
   - Возможность настройки под конкретные нужды

### Что нужно для запуска:

1. **Сервер:**
   - Мощный компьютер с GPU
   - Достаточно места на диске
   - Стабильное сетевое подключение

2. **Данные:**
   - Доступ к вики
   - Доступ к GitLab
   - Доступ к трекеру задач

3. **Настройка:**
   - Установка необходимого ПО
   - Настройка доступа к данным
   - Настройка обновления знаний

## Содержание
1. [Обзор системы](#обзор-системы)
2. [Требования к серверу](#требования-к-серверу)
3. [Выбор модели](#выбор-модели)
4. [Архитектура системы](#архитектура-системы)
5. [Развертывание](#развертывание)
6. [Подготовка данных](#подготовка-данных)
7. [Обновление контекста](#обновление-контекста)
8. [Полезные ссылки](#полезные-ссылки)

## Обзор системы

Система предназначена для создания локальной LLM (Large Language Model) с доступом к внутренней документации, коду и задачам. Основные компоненты:

- Локальная LLM модель (DeepSeek)
- Векторная база данных (ChromaDB)
- Система сбора и обработки данных
- Веб-интерфейс для взаимодействия

## Требования к серверу

### Минимальная конфигурация (для 7B модели):
- CPU: 8 ядер
- RAM: 32GB
- GPU: NVIDIA GPU с 16GB VRAM
- SSD: 100GB
- Сеть: 1Gbps

### Рекомендуемая конфигурация (для 13B модели):
- CPU: 16 ядер
- RAM: 64GB
- GPU: NVIDIA GPU с 24GB VRAM
- SSD: 200GB
- Сеть: 1Gbps

## Выбор модели

### DeepSeek
- DeepSeek Coder (1.3B, 6.7B, 33B) - для работы с кодом
- DeepSeek Chat (7B, 13B, 33B, 67B) - для общих задач

### Преимущества DeepSeek:
1. Отличная производительность в кодировании
2. Хорошее понимание контекста
3. Поддержка длинных контекстов (до 32K токенов)
4. Различные размеры моделей

## Архитектура системы

### Варианты архитектуры:

1. **RAG (Retrieval Augmented Generation)**
   - Использует векторную БД для хранения эмбеддингов
   - Легкое обновление контекста
   - Меньше потребление памяти
   - Возможность точного указания источников

2. **Fine-tuning**
   - Полное дообучение модели
   - Более глубокое понимание контекста
   - Сложнее обновлять знания
   - Требует больше ресурсов

## Развертывание

### 1. Docker Compose конфигурация

```yaml
version: '3.8'
services:
  deepseek:
    image: deepseek/deepseek-coder:latest
    ports:
      - "8000:8000"
    volumes:
      - ./models:/app/models
      - ./data:/app/data
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - MODEL_PATH=/app/models/deepseek-coder-7b-instruct
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  chromadb:
    image: chromadb/chroma:latest
    ports:
      - "8001:8000"
    volumes:
      - ./chroma_data:/chroma/chroma
    environment:
      - ALLOW_RESET=true
```

### 2. Основные компоненты системы

#### Сборщик данных (data_collector.py)
```python
class DataCollector:
    def __init__(self):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len
        )
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )
    
    def collect_gitlab_data(self, repo_url, branch="main"):
        loader = GitLoader(
            repo_url=repo_url,
            branch=branch,
            file_filter=lambda file_path: file_path.endswith(('.md', '.txt', '.py', '.js', '.java'))
        )
        return loader.load()
    
    def collect_wiki_data(self, wiki_url, api_token):
        # Реализация сбора данных из вики
        pass
    
    def collect_tracker_data(self, tracker_url, api_token):
        # Реализация сбора данных из трекера
        pass
```

#### Менеджер векторной БД (vector_store_manager.py)
```python
class VectorStoreManager:
    def __init__(self):
        load_dotenv()
        self.collector = DataCollector()
    
    def update_all_sources(self):
        # Реализация обновления всех источников
        pass
```

#### Сервис DeepSeek (deepseek_service.py)
```python
class DeepSeekService:
    def __init__(self):
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )
        self.llm = HuggingFaceHub(
            repo_id="deepseek-ai/deepseek-coder-7b-instruct",
            model_kwargs={"temperature": 0.7}
        )
```

### 3. Зависимости (requirements.txt)
```text
langchain==0.0.200
chromadb==0.4.15
sentence-transformers==2.2.2
gitpython==3.1.40
requests==2.31.0
beautifulsoup4==4.12.2
python-dotenv==1.0.0
schedule==1.2.0
```

## Подготовка данных

### 1. Сбор данных из различных источников:
- GitLab: код и документация
- Вики: внутренняя документация
- Трекер: задачи и их описания

### 2. Обработка данных:
- Разделение на чанки
- Создание эмбеддингов
- Сохранение в векторную БД

### 3. Конфигурация (.env)
```bash
GITLAB_REPO_URL=https://gitlab.com/your-org/your-repo
GITLAB_BRANCH=main
WIKI_URL=https://your-wiki-url
WIKI_API_TOKEN=your-wiki-token
TRACKER_URL=https://your-tracker-url
TRACKER_API_TOKEN=your-tracker-token
```

## Обновление контекста

### 1. Периодическое обновление:
- Настройка расписания обновлений
- Проверка изменений в источниках
- Переиндексация измененных данных

### 2. Мониторинг:
- Отслеживание использования ресурсов
- Проверка качества ответов
- Ведение логов

## Полезные ссылки

1. [DeepSeek GitHub](https://github.com/deepseek-ai/deepseek-coder)
2. [ChromaDB GitHub](https://github.com/chroma-core/chroma)
3. [LangChain Documentation](https://python.langchain.com/docs/get_started/introduction)
4. [Sentence Transformers Documentation](https://www.sbert.net/)

## Решения RAG с DeepSeek

### 1. Вариант с LangChain

```python
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.llms import HuggingFaceHub
from langchain.chains import RetrievalQA
from langchain.document_loaders import DirectoryLoader

class DeepSeekRAG:
    def __init__(self):
        # Инициализация эмбеддингов
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )
        
        # Инициализация DeepSeek
        self.llm = HuggingFaceHub(
            repo_id="deepseek-ai/deepseek-coder-7b-instruct",
            model_kwargs={"temperature": 0.7}
        )
        
        # Инициализация векторной БД
        self.vectorstore = Chroma(
            persist_directory="./chroma_data",
            embedding_function=self.embeddings
        )
    
    def load_documents(self, directory):
        loader = DirectoryLoader(directory)
        documents = loader.load()
        self.vectorstore.add_documents(documents)
    
    def query(self, question):
        qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vectorstore.as_retriever(
                search_kwargs={"k": 3}
            )
        )
        return qa_chain.run(question)
```

### 2. Вариант с LlamaIndex

```python
from llama_index import VectorStoreIndex, SimpleDirectoryReader
from llama_index.llms import HuggingFaceLLM
from llama_index.embeddings import HuggingFaceEmbedding

class DeepSeekLlamaIndex:
    def __init__(self):
        # Инициализация LLM
        self.llm = HuggingFaceLLM(
            repo_id="deepseek-ai/deepseek-coder-7b-instruct",
            tokenizer_name="deepseek-ai/deepseek-coder-7b-instruct",
            max_new_tokens=512,
            temperature=0.7
        )
        
        # Инициализация эмбеддингов
        self.embed_model = HuggingFaceEmbedding(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )
    
    def load_documents(self, directory):
        documents = SimpleDirectoryReader(directory).load_data()
        self.index = VectorStoreIndex.from_documents(
            documents,
            embed_model=self.embed_model
        )
    
    def query(self, question):
        query_engine = self.index.as_query_engine()
        return query_engine.query(question)
```

### 3. Вариант с собственной реализацией

```python
import torch
from transformers import AutoTokenizer, AutoModel
from sentence_transformers import SentenceTransformer
from chromadb import Client, Settings

class CustomDeepSeekRAG:
    def __init__(self):
        # Инициализация DeepSeek
        self.tokenizer = AutoTokenizer.from_pretrained(
            "deepseek-ai/deepseek-coder-7b-instruct"
        )
        self.model = AutoModel.from_pretrained(
            "deepseek-ai/deepseek-coder-7b-instruct",
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        # Инициализация эмбеддингов
        self.embedding_model = SentenceTransformer(
            "sentence-transformers/all-MiniLM-L6-v2"
        )
        
        # Инициализация ChromaDB
        self.client = Client(Settings(
            persist_directory="./chroma_data",
            anonymized_telemetry=False
        ))
        self.collection = self.client.create_collection("documents")
    
    def add_documents(self, texts):
        embeddings = self.embedding_model.encode(texts)
        self.collection.add(
            embeddings=embeddings.tolist(),
            documents=texts,
            ids=[f"doc_{i}" for i in range(len(texts))]
        )
    
    def query(self, question, k=3):
        query_embedding = self.embedding_model.encode(question)
        results = self.collection.query(
            query_embeddings=[query_embedding.tolist()],
            n_results=k
        )
        return results
```

## Дополнительные ресурсы

### Репозитории и примеры:
1. [DeepSeek Examples](https://github.com/deepseek-ai/deepseek-coder/)
2. [LangChain Examples](https://github.com/langchain-ai/langchain/)
3. [LlamaIndex Examples](https://github.com/run-llama/llama_index/)

### Документация по RAG:
1. [LangChain RAG Documentation](https://python.langchain.com/docs/tutorials/rag)
2. [LlamaIndex Documentation](https://docs.llamaindex.ai/)
3. [ChromaDB Documentation](https://docs.trychroma.com/)


### Бенчмарки и сравнения:
1. [RAG Systems Overview](https://www.pinecone.io/learn/retrieval-augmented-generation/)

### Дополнительные материалы:
1. [DeepSeek Model Card](https://huggingface.co/deepseek-ai/deepseek-coder-7b-instruct)
2. [LangChain Cookbook](https://python.langchain.com/api_reference/deepseek/chat_models/langchain_deepseek.chat_models.ChatDeepSeek.html#langchain_deepseek.chat_models.ChatDeepSeek)